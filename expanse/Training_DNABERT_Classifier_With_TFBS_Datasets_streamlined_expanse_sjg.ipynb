{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfoAktdfUsl1"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSKLjlh-HmIi"
      },
      "source": [
        "Installing Bio, transformers, genomic-benchmarks, and datasets packages.  The Bio package is from Biopython; transformers package for machine learning (pytorch, tensorflow); genomic-benchmarks and datasets from ML-Bioinfo-CEITEC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTrn2sqTOGNV",
        "outputId": "43865a08-55b5-46d0-8a58-652727edc99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for genomic-benchmarks (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# already set up on Expanse\n",
        "# pip install -qq Bio transformers genomic-benchmarks datasets transformers[torch] evaluate scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set kmer and model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmer = 6\n",
        "model_used = \"armheb/DNA_bert_\" + str(kmer)\n",
        "bs = 64\n",
        "epochs = 4\n",
        "warmup = 100\n",
        "maxstp = 1000\n",
        "lr = 8e-5\n",
        "\n",
        "run_name = \"run\" + \"1-2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se-0kxjeM_Li"
      },
      "source": [
        "Set output path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SC8phKyD_uH",
        "outputId": "265e7c82-3edc-42ff-932f-5b97f49cab68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# set output path\n",
        "\n",
        "path_prefix = \"/expanse/lustre/projects/nji102/sgriesmer/\"\n",
        "# stdpath = path_prefix + \"DNABERT/output/\" + \"optimization\" + \"/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2jXeMkTnSE8"
      },
      "source": [
        "Run model on next 10 datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "c52111082c994800ad2e73f9a1006898",
            "c3bf40e9123646baa0252cee30f92ee9",
            "349d2d68ac8040098143dee02fd4cb25",
            "4fad3f6bbdd943fca89b2aef05ef1b50",
            "154f733234094324a6bc77aa626d838e",
            "eb2509a5f9404b3b9f1132e965b5ffab",
            "fbde223bd28b4a66b0238a3ebb751219",
            "a69fea58420a4f1fbd65d6660145c2b0"
          ]
        },
        "id": "9eGhbThTEVTP",
        "outputId": "7f4cf3b2-61a8-4f5b-c264-69ea76c84537"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c52111082c994800ad2e73f9a1006898",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3bf40e9123646baa0252cee30f92ee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/359M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at armheb/DNA_bert_6 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349d2d68ac8040098143dee02fd4cb25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fad3f6bbdd943fca89b2aef05ef1b50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "154f733234094324a6bc77aa626d838e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb2509a5f9404b3b9f1132e965b5ffab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/77988 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbde223bd28b4a66b0238a3ebb751219",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25996 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8646' max='9752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8646/9752 28:48 < 03:41, 5.00 it/s, Epoch 3.55/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.296700</td>\n",
              "      <td>0.253589</td>\n",
              "      <td>0.902293</td>\n",
              "      <td>0.898950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.270653</td>\n",
              "      <td>0.905716</td>\n",
              "      <td>0.906788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.173200</td>\n",
              "      <td>0.287622</td>\n",
              "      <td>0.901216</td>\n",
              "      <td>0.904265</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a69fea58420a4f1fbd65d6660145c2b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9752' max='9752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9752/9752 32:54, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.296700</td>\n",
              "      <td>0.253589</td>\n",
              "      <td>0.902293</td>\n",
              "      <td>0.898950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.270653</td>\n",
              "      <td>0.905716</td>\n",
              "      <td>0.906788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.173200</td>\n",
              "      <td>0.287622</td>\n",
              "      <td>0.901216</td>\n",
              "      <td>0.904265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.117900</td>\n",
              "      <td>0.407823</td>\n",
              "      <td>0.889945</td>\n",
              "      <td>0.895473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# set up dataframe to capture testing results\n",
        "\n",
        "test_column_names = ['test_loss', 'test_accuracy', 'test_precision', 'test_recall',\n",
        "                     'test_f1', 'test_matthews_correlation', 'test_runtime']\n",
        "testing_results_df = pd.DataFrame(columns=test_column_names)\n",
        "\n",
        "# initialize parameters\n",
        "\n",
        "for fname in [\n",
        "#  \"BroadDnd41CtcfUniPk-ran.csv\",\n",
        "#  \"BroadDnd41Ezh239875UniPk-ran.csv\",\n",
        "#  \"BroadGm12878CtcfUniPk-ran.csv\",\n",
        "#  \"BroadGm12878Ezh239875UniPk-ran.csv\",\n",
        "#  \"BroadH1hescChd1a301218aUniPk-ran.csv\",\n",
        "#  \"BroadH1hescCtcfUniPk-ran.csv\",\n",
        "#  \"BroadH1hescEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadH1hescJarid1aab26049UniPk-ran.csv\",\n",
        "#  \"BroadH1hescRbbp5a300109aUniPk-ran.csv\",\n",
        "#  \"BroadHelas3CtcfUniPk-ran.csv\",\n",
        "#  \"BroadHelas3Ezh239875UniPk-ran.csv\",\n",
        "#  \"BroadHelas3Pol2bUniPk-ran.csv\",\n",
        "#  \"BroadHepg2CtcfUniPk-ran.csv\",\n",
        "#  \"BroadHepg2Ezh239875UniPk-ran.csv\",\n",
        "#  \"BroadHmecCtcfUniPk-ran.csv\",\n",
        "#  \"BroadHmecEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadHsmmCtcfUniPk-ran.csv\",\n",
        "#  \"BroadHsmmEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadHsmmtCtcfUniPk-ran.csv\",\n",
        "#  \"BroadHsmmtEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadHuvecCtcfUniPk-ran.csv\",\n",
        "#  \"BroadHuvecEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadHuvecPol2bUniPki-ran.csv\",\n",
        "#  \"BroadK562Chd1a301218aUniPk-ran.csv\",\n",
        "#  \"BroadK562CtcfUniPk-ran.csv\",\n",
        "#  \"BroadK562Ezh239875UniPk-ran.csv\",\n",
        "#  \"BroadK562Hdac1sc6298UniPk-ran.csv\",\n",
        "#  \"BroadK562Hdac2a300705aUniPk-ran.csv\",\n",
        "#  \"BroadK562Hdac6a301341aUniPk-ran.csv\",\n",
        "#  \"BroadK562P300UniPk-ran.csv\",\n",
        "#  \"BroadK562Phf8a301772aUniPk-ran.csv\",\n",
        "#  \"BroadK562Plu1UniPk-ran.csv\",\n",
        "#  \"BroadK562Pol2bUniPk-ran.csv\",\n",
        "#  \"BroadK562Rbbp5a300109aUniPk-ran.csv\",\n",
        "#  \"BroadK562Sap3039731UniPk-ran.csv\",\n",
        "#  \"BroadNhaCtcfUniPk-ran.csv\",\n",
        "#  \"BroadNhaEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadNhdfadCtcfUniPk-ran.csv\",\n",
        "#  \"BroadNhdfadEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadNhekCtcfUniPk-ran.csv\",\n",
        "#  \"BroadNhekEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadNhekPol2bUniPk-ran.csv\",\n",
        "#  \"BroadNhlfCtcfUniPk-ran.csv\",\n",
        "#  \"BroadNhlfEzh239875UniPk-ran.csv\",\n",
        "#  \"BroadOsteoblCtcfUniPk-ran.csv\",\n",
        "#  \"HaibA549Atf3V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Bcl3V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Creb1sc240V0416102Dex100nmUniPk-ran.csv\",\n",
        "#  \"HaibA549Ctcfsc5916Pcr1xDex100nmUniPk-ran.csv\",\n",
        "#  \"HaibA549Ctcfsc5916Pcr1xEtoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Elf1V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Ets1V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Fosl2V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Foxa1V0416102Dex100nmUniPk-ran.csv\",\n",
        "#  \"HaibA549GabpV0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549GrPcr1xDex500pmUniPk-ran.csv\",\n",
        "#  \"HaibA549GrPcr1xDex50nmUniPk-ran.csv\",\n",
        "#  \"HaibA549GrPcr1xDex5nmUniPk-ran.csv\",\n",
        "#  \"HaibA549GrPcr2xDex100nmUniPk-ran.csv\",\n",
        "#  \"HaibA549NrsfV0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549P300V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Pol2Pcr2xDex100nmUniPk-ran.csv\",\n",
        "#  \"HaibA549Pol2Pcr2xEtoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Sin3ak20V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Six5V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Taf1V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Tcf12V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Usf1Pcr1xDex100nmUniPk-ran.csv\",\n",
        "#  \"HaibA549Usf1Pcr1xEtoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Usf1V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Yy1cV0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibA549Zbtb33V0422111Etoh02UniPk-ran.csv\",\n",
        "#  \"HaibEcc1CtcfcV0416102Dm002p1hUniPk-ran.csv\",\n",
        "#  \"HaibEcc1EraaV0416102Bpa1hUniPk-ran.csv\",\n",
        "#  \"HaibEcc1EralphaaV0416102Est10nm1hUniPk-ran.csv\",\n",
        "#  \"HaibEcc1EralphaaV0416102Gen1hUniPk-ran.csv\",\n",
        "#  \"HaibEcc1Foxa1sc6553V0416102Dm002p1hUniPk-ran.csv\",\n",
        "#  \"HaibEcc1GrV0416102Dex100nmUniPk-ran.csv\",\n",
        "#  \"HaibEcc1Pol2V0416102Dm002p1hUniPk-ran.csv\"\n",
        "]:\n",
        "\n",
        "  dsname = path_prefix + \"DNABERT/Datasets/tfbs/\" + fname\n",
        "  tfbs_dataset = pd.read_csv(dsname, sep=',')\n",
        "\n",
        "# change standard error and output to saved output file\n",
        "\n",
        "  stdpath = path_prefix + \"DNABERT/output/\" + fname.split(\".\")[0] + \"/\"\n",
        "\n",
        "  try:\n",
        "    os.makedirs(stdpath)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "\n",
        "  stdpath_out = stdpath + \"stdout.txt\"\n",
        "  stdpath_err = stdpath + \"stderr.txt\"\n",
        "\n",
        "  sys.stdout = open(stdpath_out, 'w')\n",
        "  sys.stderr = open(stdpath_err, 'w')\n",
        "\n",
        "# print dataset shape and initial values\n",
        "\n",
        "  print(\"tfbs dataset:\", tfbs_dataset.shape, file=sys.stdout)\n",
        "  print(\"tfbs dataset initial values:\", tfbs_dataset.head(), file=sys.stdout)\n",
        "\n",
        "# reformat input\n",
        "\n",
        "  column_names = [\"labels\", \"seq\"]\n",
        "  tfbs_dataset_res = pd.DataFrame(columns=column_names)\n",
        "\n",
        "  j = 1\n",
        "  for i in range(tfbs_dataset.count()['names']):\n",
        "    name = tfbs_dataset['names'][i]\n",
        "    pos_seq = tfbs_dataset['seq'][i]\n",
        "    neg_seq = tfbs_dataset['neg_seq'][i]\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    tfbs_dataset_res.loc[j] = [pos_label, pos_seq]\n",
        "    tfbs_dataset_res.loc[j+1] = [neg_label, neg_seq]\n",
        "    j+=2\n",
        "\n",
        "  X = tfbs_dataset_res['seq']\n",
        "  y = tfbs_dataset_res['labels']\n",
        "  X_size = len(X)\n",
        "\n",
        "# test range is 500 odd-numbered pairs of sequences\n",
        "# training range is 500 even-numbered pairs plus remainder of the sequences over 1000\n",
        "\n",
        "  test_range = list(range(3,1001,4)) + list(range(4,1001,4))\n",
        "  test_range.sort()\n",
        "  train_range = list(range(1,1001,4)) + list(range(2,1001,4))\n",
        "  train_range.sort()\n",
        "  train_range = train_range + list(range(1001,X_size))\n",
        "\n",
        "  X_test = X.loc[test_range]\n",
        "  X_train = X.loc[train_range]\n",
        "  y_test = y.loc[test_range]\n",
        "  y_train = y.loc[train_range]\n",
        "\n",
        "# print size of training and test sets\n",
        "\n",
        "  print(\"X_train shape:\", X_train.shape, file=sys.stdout)\n",
        "  print(\"y_train shape:\", y_train.shape, file=sys.stdout)\n",
        "  print(\"X_test shape:\", X_test.shape, file=sys.stdout)\n",
        "  print(\"y_test shape:\", y_test.shape, file=sys.stdout)\n",
        "\n",
        "# tokenization\n",
        "\n",
        "  def kmers_stride1(s, k=kmer):\n",
        "    return [s[i:i + k] for i in range(0, len(s)-k+1)]\n",
        "\n",
        "# load pre-trained model\n",
        "\n",
        "  model_cls = AutoModelForSequenceClassification.from_pretrained(model_used, num_labels=2)\n",
        "  params = list(model_cls.named_parameters())\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_used)\n",
        "\n",
        "# reformat data to Hugging Face Dataset format from pandas\n",
        "\n",
        "  ds_Xy_train = pd.concat([y_train, X_train], axis=1)\n",
        "  ds_Xy_test = pd.concat([y_test, X_test], axis=1)\n",
        "\n",
        "  Dataset_Xy_train = Dataset.from_pandas(ds_Xy_train)\n",
        "  Dataset_Xy_test = Dataset.from_pandas(ds_Xy_test)\n",
        "  Dataset_Xy_train, Dataset_Xy_test\n",
        "\n",
        "  def tok_func(x): return tokenizer(\" \".join(kmers_stride1(x[\"seq\"])))\n",
        "\n",
        "  Dataset_Xy_train_tok = Dataset_Xy_train.map(tok_func, batched=False)\n",
        "  new_column = [\"train\"] * len(Dataset_Xy_train_tok)\n",
        "  Dataset_Xy_train_tok = Dataset_Xy_train_tok.add_column(\"dset\", new_column)\n",
        "\n",
        "  Dataset_Xy_test_tok = Dataset_Xy_test.map(tok_func, batched=False)\n",
        "  new_column = [\"test\"] * len(Dataset_Xy_test_tok)\n",
        "  Dataset_Xy_test_tok = Dataset_Xy_test_tok.add_column(\"dset\", new_column)\n",
        "\n",
        "  dds = DatasetDict({\n",
        "    'train': Dataset_Xy_train_tok,\n",
        "    'test': Dataset_Xy_test_tok\n",
        "  })\n",
        "\n",
        "# switch to GPU\n",
        "\n",
        "  if torch.cuda.device_count() > 0:\n",
        "    model_cls.to('cuda')\n",
        "\n",
        "# train model\n",
        "\n",
        "  args = TrainingArguments('outputs', learning_rate=lr, warmup_steps=warmup, max_steps=maxstp, lr_scheduler_type='cosine', fp16=True,\n",
        "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "    metric = evaluate.combine([\"accuracy\",\"precision\",\"recall\",\"f1\",\"matthews_correlation\"])\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  trainer = Trainer(model_cls, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "  trainer.train();\n",
        "\n",
        "# save model\n",
        "\n",
        "  fpath = path_prefix + \"DNABERT/Output_Models/\" + fname.split(\".\")[0] + \"/\"\n",
        "  print(fpath, file=sys.stdout)\n",
        "  trainer.save_model(fpath)\n",
        "\n",
        "# test model\n",
        "\n",
        "  eval_preds = trainer.predict(dds['test'])\n",
        "\n",
        "  print(eval_preds, file=sys.stdout)\n",
        "\n",
        "  tfbs_ds = fname.split(\".\")[0] \n",
        "  testing_results_df.loc[tfbs_ds] = [eval_preds.metrics['test_loss'],\n",
        "                                  eval_preds.metrics['test_accuracy'],\n",
        "                                  eval_preds.metrics['test_precision'],\n",
        "                                  eval_preds.metrics['test_recall'],\n",
        "                                  eval_preds.metrics['test_f1'],\n",
        "                                  eval_preds.metrics['test_matthews_correlation'],\n",
        "                                  eval_preds.metrics['test_runtime']]\n",
        "  \n",
        "outpath = path_prefix + \"DNABERT/output/\" + run_name + \".csv\"\n",
        "testing_results_df.to_csv(outpath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNxH_dZNx7Zh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
