{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfoAktdfUsl1"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSKLjlh-HmIi"
   },
   "source": [
    "Installing Bio, transformers, genomic-benchmarks, and datasets packages.  The Bio package is from Biopython; transformers package for machine learning (pytorch, tensorflow); genomic-benchmarks and datasets from ML-Bioinfo-CEITEC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTrn2sqTOGNV",
    "outputId": "43865a08-55b5-46d0-8a58-652727edc99f"
   },
   "outputs": [],
   "source": [
    "# already set up on Expanse\n",
    "# pip install Bio transformers genomic-benchmarks datasets transformers[torch] evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set kmer and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 6\n",
    "#model_used = \"armheb/DNA_bert_\" + str(kmer)\n",
    "model_used = \"zhihan1996/DNABERT-2-117M\"\n",
    "train_bs = 8\n",
    "eval_bs = 64\n",
    "epochs = 3\n",
    "warmup = 30\n",
    "lr = 3e-5\n",
    "save_steps = 200\n",
    "eval_steps = 200\n",
    "save_total_limit = 3\n",
    "\n",
    "\n",
    "\n",
    "run_name = \"run-\" + \"gue-test-db2-git-19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se-0kxjeM_Li"
   },
   "source": [
    "Set output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SC8phKyD_uH",
    "outputId": "265e7c82-3edc-42ff-932f-5b97f49cab68"
   },
   "outputs": [],
   "source": [
    "# set output path\n",
    "\n",
    "path_prefix = \"/expanse/lustre/projects/nji102/sgriesmer/\"\n",
    "# stdpath = path_prefix + \"DNABERT/output/\" + \"optimization\" + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2jXeMkTnSE8"
   },
   "source": [
    "Run model on next 10 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547,
     "referenced_widgets": [
      "c52111082c994800ad2e73f9a1006898",
      "c3bf40e9123646baa0252cee30f92ee9",
      "349d2d68ac8040098143dee02fd4cb25",
      "4fad3f6bbdd943fca89b2aef05ef1b50",
      "154f733234094324a6bc77aa626d838e",
      "eb2509a5f9404b3b9f1132e965b5ffab",
      "fbde223bd28b4a66b0238a3ebb751219",
      "a69fea58420a4f1fbd65d6660145c2b0"
     ]
    },
    "id": "9eGhbThTEVTP",
    "outputId": "7f4cf3b2-61a8-4f5b-c264-69ea76c84537"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'classifier.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366261c1a2a044a595d76a4cf81649be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea3723486d74e0089924b388f365625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12144' max='12144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12144/12144 14:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.522505</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.811736</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.730473</td>\n",
       "      <td>0.518662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.496751</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.788497</td>\n",
       "      <td>0.550743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.626322</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.824607</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.510418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.510778</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.677918</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.796036</td>\n",
       "      <td>0.558132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559896</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.621859</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.481427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.515316</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.734219</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.802178</td>\n",
       "      <td>0.576115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495993</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.713154</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.795756</td>\n",
       "      <td>0.557474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.514504</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.555538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.549182</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.662534</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.784666</td>\n",
       "      <td>0.529137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.509221</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.730956</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.807520</td>\n",
       "      <td>0.586277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565792</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.638270</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.771180</td>\n",
       "      <td>0.496188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487792</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.769074</td>\n",
       "      <td>0.546316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.486366</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.731511</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.811052</td>\n",
       "      <td>0.593952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.513579</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.721865</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.800357</td>\n",
       "      <td>0.569204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.526080</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.790254</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.767490</td>\n",
       "      <td>0.548861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.483248</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.748709</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>0.585737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.704992</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.802756</td>\n",
       "      <td>0.572491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.480788</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.587698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465431</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.712991</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.812392</td>\n",
       "      <td>0.596159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.529925</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.707391</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.806535</td>\n",
       "      <td>0.581783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492703</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.722309</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.811569</td>\n",
       "      <td>0.594112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.485171</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.815835</td>\n",
       "      <td>0.604615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.507353</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.801909</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.731230</td>\n",
       "      <td>0.512773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489874</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.752182</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.803355</td>\n",
       "      <td>0.584261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.485023</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.603023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.510396</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.555136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465280</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.739566</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.806187</td>\n",
       "      <td>0.585594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.469734</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.731200</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.812444</td>\n",
       "      <td>0.596956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.712991</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.812392</td>\n",
       "      <td>0.596159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.545483</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.630573</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.770428</td>\n",
       "      <td>0.498999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487614</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.751812</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.559031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474435</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.783890</td>\n",
       "      <td>0.560363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.470464</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.735669</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>0.612407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.477748</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.770073</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.805344</td>\n",
       "      <td>0.594747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.466224</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.783951</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.772819</td>\n",
       "      <td>0.552217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474430</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.742097</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.810173</td>\n",
       "      <td>0.594250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484364</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.707646</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.808912</td>\n",
       "      <td>0.587753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512267</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.667582</td>\n",
       "      <td>0.972000</td>\n",
       "      <td>0.791531</td>\n",
       "      <td>0.548327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.480550</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.734959</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.810762</td>\n",
       "      <td>0.593923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471342</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.793804</td>\n",
       "      <td>0.575254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.459838</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.769088</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.796528</td>\n",
       "      <td>0.579589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.486297</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.808874</td>\n",
       "      <td>0.587878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.482422</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.730586</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.815208</td>\n",
       "      <td>0.603066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.467381</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.805147</td>\n",
       "      <td>0.585134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474269</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.745704</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.802218</td>\n",
       "      <td>0.579851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463503</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.740066</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.809783</td>\n",
       "      <td>0.592969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.455193</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.736409</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.807588</td>\n",
       "      <td>0.587613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.476690</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.729773</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.806798</td>\n",
       "      <td>0.584511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.450819</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.749135</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.803340</td>\n",
       "      <td>0.583139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.459158</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.745704</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.802218</td>\n",
       "      <td>0.579851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449146</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.745704</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.802218</td>\n",
       "      <td>0.579851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.447331</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.752197</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.800748</td>\n",
       "      <td>0.579545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439873</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.747871</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.807728</td>\n",
       "      <td>0.591016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.452171</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.735910</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.815343</td>\n",
       "      <td>0.603952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448842</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.735537</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.805430</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.456510</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.725857</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.816112</td>\n",
       "      <td>0.604908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.453336</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.721966</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.606310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446225</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.734959</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.810762</td>\n",
       "      <td>0.593923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449215</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.810036</td>\n",
       "      <td>0.592157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448694</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.732689</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.811775</td>\n",
       "      <td>0.595707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'classifier.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a47a78a74224cf2911b5023195524ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c71dca05b04d91917ccd57ef0bd861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11502' max='11502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11502/11502 13:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.584368</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.667133</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.529447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495194</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.701937</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.804441</td>\n",
       "      <td>0.576780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.477078</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.815109</td>\n",
       "      <td>0.628045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.530672</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.682270</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.798340</td>\n",
       "      <td>0.563544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686841</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.077693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597628</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.790584</td>\n",
       "      <td>0.546377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.552032</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.644180</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.775478</td>\n",
       "      <td>0.507576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.498330</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.757167</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.821592</td>\n",
       "      <td>0.620834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.527511</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.707831</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.584326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.543411</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.660690</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.782041</td>\n",
       "      <td>0.521820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.570227</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.768827</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.819795</td>\n",
       "      <td>0.620286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.479048</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.810484</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.616020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.466108</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.749191</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.828265</td>\n",
       "      <td>0.633906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.454424</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.715994</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.819504</td>\n",
       "      <td>0.614145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.498099</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.805825</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.630284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.491197</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.706845</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.810580</td>\n",
       "      <td>0.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.445953</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.826617</td>\n",
       "      <td>0.635733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439820</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.779964</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.823418</td>\n",
       "      <td>0.630404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428144</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.799235</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.626663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458072</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.814516</td>\n",
       "      <td>0.632081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.440545</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.776786</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.624513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.456068</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.750831</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.820327</td>\n",
       "      <td>0.616974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484311</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.718028</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.811140</td>\n",
       "      <td>0.592940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487216</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.757119</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.824066</td>\n",
       "      <td>0.625891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.451482</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.821012</td>\n",
       "      <td>0.632993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427082</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.776248</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.834413</td>\n",
       "      <td>0.650594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426772</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.796992</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.633298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428904</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.832872</td>\n",
       "      <td>0.646976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421399</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.784698</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.644978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.530227</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.647682</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.779283</td>\n",
       "      <td>0.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.455809</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.745161</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.626305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.479902</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.802676</td>\n",
       "      <td>0.573935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.433276</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.825455</td>\n",
       "      <td>0.628702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.452857</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.826855</td>\n",
       "      <td>0.630364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.444441</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.757025</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.828959</td>\n",
       "      <td>0.636186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430153</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.801512</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.824101</td>\n",
       "      <td>0.639076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425484</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.797794</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.831418</td>\n",
       "      <td>0.650524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428372</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.777391</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.831628</td>\n",
       "      <td>0.645301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.431731</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.722137</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.612157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.831793</td>\n",
       "      <td>0.644729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.437357</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.732087</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.823117</td>\n",
       "      <td>0.621595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.422627</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.771626</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.827458</td>\n",
       "      <td>0.635784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.759664</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.825571</td>\n",
       "      <td>0.629466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419375</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.781139</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.826742</td>\n",
       "      <td>0.636916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428105</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.760469</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.827712</td>\n",
       "      <td>0.634046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.436555</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.768581</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.647048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.416923</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.782837</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.834734</td>\n",
       "      <td>0.652613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.409528</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.782230</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.836127</td>\n",
       "      <td>0.655216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.416853</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.833176</td>\n",
       "      <td>0.650862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.415174</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.834750</td>\n",
       "      <td>0.654573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413176</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.783451</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.650040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412390</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.779130</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.833488</td>\n",
       "      <td>0.649347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.414011</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.761269</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.829845</td>\n",
       "      <td>0.638644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413372</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.770940</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.831336</td>\n",
       "      <td>0.643365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407901</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.830855</td>\n",
       "      <td>0.643477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407904</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.782074</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.832554</td>\n",
       "      <td>0.648202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.408026</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.777003</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.830540</td>\n",
       "      <td>0.643082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'classifier.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9d592d843c40939f42f17560702511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee7e5e0bea3440c993dab40da5053fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7125' max='7125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7125/7125 08:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.601647</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.647929</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.427350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628153</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.738583</td>\n",
       "      <td>0.399209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652305</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.631068</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.745291</td>\n",
       "      <td>0.421398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568397</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.714571</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.715285</td>\n",
       "      <td>0.430001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589565</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.713568</td>\n",
       "      <td>0.430022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565976</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.687395</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.747032</td>\n",
       "      <td>0.454275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.556491</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.678072</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.762905</td>\n",
       "      <td>0.477965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.611457</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.659639</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.448830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565338</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.606000</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.420496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572634</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>0.461479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.561976</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.688995</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.490072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.680818</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.762324</td>\n",
       "      <td>0.478023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.577429</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.641061</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.754934</td>\n",
       "      <td>0.447956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.563942</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.720472</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>0.448057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.608285</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.713987</td>\n",
       "      <td>0.453603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.606902</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.656934</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.462847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.561306</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.696763</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.752530</td>\n",
       "      <td>0.469157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.554339</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.711775</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.757717</td>\n",
       "      <td>0.486656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.557512</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562687</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.752830</td>\n",
       "      <td>0.479465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.582385</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.721612</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.753346</td>\n",
       "      <td>0.486061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562284</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.764912</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.550883</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.749474</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.730256</td>\n",
       "      <td>0.474594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.550550</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.733075</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.745329</td>\n",
       "      <td>0.482279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.688103</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.762923</td>\n",
       "      <td>0.482586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.545095</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.722426</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.752874</td>\n",
       "      <td>0.485885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.569340</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.755005</td>\n",
       "      <td>0.488351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541433</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.739645</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744786</td>\n",
       "      <td>0.486048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.548747</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.757776</td>\n",
       "      <td>0.489658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.532409</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.497291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.532002</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.690590</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.768412</td>\n",
       "      <td>0.494208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540088</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.774483</td>\n",
       "      <td>0.511227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524904</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.721340</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.766635</td>\n",
       "      <td>0.506569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.523068</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.728302</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.749515</td>\n",
       "      <td>0.484874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.523633</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.723519</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.762535</td>\n",
       "      <td>0.501268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'classifier.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b00e5ba83d4e3483555db0e264210c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27294 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3842678892484916b24c74e131c141c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10236' max='10236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10236/10236 11:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.713241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704836</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706603</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696534</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.703607</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696996</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693869</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.701174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696386</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.711295</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.713380</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704105</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693339</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690672</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693065</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697998</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693604</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694402</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694066</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694087</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693604</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696699</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.712402</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693508</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.695004</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691294</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693245</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720686</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697521</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693850</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693604</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693550</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694782</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693271</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693328</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694337</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694596</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696119</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704146</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693297</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693352</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693604</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693348</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693123</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'classifier.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3606604f31e2479db2cb6777bd0f910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598d35e756254524aa0818b8316e87fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5401' max='7125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5400/7125 06:40 < 02:07, 13.49 it/s, Epoch 2.27/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505107</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.776447</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.554001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.677233</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.787270</td>\n",
       "      <td>0.533820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.523685</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.547382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.579821</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.569970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.481087</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.765027</td>\n",
       "      <td>0.578419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541157</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.832918</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.741398</td>\n",
       "      <td>0.544786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471018</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.776753</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.808061</td>\n",
       "      <td>0.602128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.468144</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.771277</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.817669</td>\n",
       "      <td>0.617076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.819188</td>\n",
       "      <td>0.616766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448120</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.601083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.725309</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.818815</td>\n",
       "      <td>0.611398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487292</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.788497</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.818094</td>\n",
       "      <td>0.623901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.444594</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.745192</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.827402</td>\n",
       "      <td>0.631735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463021</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.801527</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.632729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474714</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.707463</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.810256</td>\n",
       "      <td>0.591222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423250</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.833652</td>\n",
       "      <td>0.654777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424457</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.822134</td>\n",
       "      <td>0.640184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.418761</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.780870</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.835349</td>\n",
       "      <td>0.653392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.432449</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.793489</td>\n",
       "      <td>0.594344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424043</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.747619</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.833628</td>\n",
       "      <td>0.646225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398342</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.804428</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.836852</td>\n",
       "      <td>0.662341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427915</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.659601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407282</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.794275</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.838527</td>\n",
       "      <td>0.662629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412370</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.825832</td>\n",
       "      <td>0.644624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430708</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.778929</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.835959</td>\n",
       "      <td>0.654218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.405771</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.814453</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.824111</td>\n",
       "      <td>0.644186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# capture testing results\n",
    "\n",
    "test_column_names = ['test_loss', 'test_accuracy', 'test_precision', 'test_recall',\n",
    "                     'test_f1', 'test_matthews_correlation', 'test_runtime']\n",
    "testing_results_df = pd.DataFrame(columns=test_column_names)\n",
    "\n",
    "# initialize parameters\n",
    "\n",
    "for fname in [\"0\",\"1\",\"2\",\"3\",\"4\"]:\n",
    "  train_dsname = path_prefix + \"DNABERT/GUE/tf/\" + fname + \"/\" + \"train.csv\"\n",
    "  test_dsname = path_prefix + \"DNABERT/GUE/tf/\" + fname + \"/\" + \"test.csv\"\n",
    "  train_set = pd.read_csv(train_dsname)\n",
    "  test_set = pd.read_csv(test_dsname)\n",
    "\n",
    "# change standard error and output to saved output file\n",
    "\n",
    "  stdpath = path_prefix + \"DNABERT/output/\" + fname.split(\".\")[0] + \"/\"\n",
    "\n",
    "  try:\n",
    "    os.makedirs(stdpath)\n",
    "  except FileExistsError:\n",
    "    pass\n",
    "\n",
    "  stdpath_out = stdpath + \"stdout.txt\"\n",
    "  stdpath_err = stdpath + \"stderr.txt\"\n",
    "\n",
    "  sys.stdout = open(stdpath_out, 'w')\n",
    "  sys.stderr = open(stdpath_err, 'w')\n",
    "\n",
    "# reformat input\n",
    "\n",
    "  X_train = train_set['sequence']\n",
    "  y_train = train_set['label']\n",
    "  X_test = test_set['sequence']\n",
    "  y_test = test_set['label']\n",
    "\n",
    "# print size of training and test sets\n",
    "\n",
    "  print(\"X_train shape:\", X_train.shape, file=sys.stdout)\n",
    "  print(\"y_train shape:\", y_train.shape, file=sys.stdout)\n",
    "  print(\"X_test shape:\", X_test.shape, file=sys.stdout)\n",
    "  print(\"y_test shape:\", y_test.shape, file=sys.stdout)\n",
    "\n",
    "# load pre-trained model\n",
    "\n",
    "  model_cls = AutoModelForSequenceClassification.from_pretrained(model_used, num_labels=2)\n",
    "  params = list(model_cls.named_parameters())\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_used, model_max_length=30, padding_side=\"right\", use_fast=True)\n",
    "\n",
    "# reformat data to Hugging Face Dataset format from pandas\n",
    "\n",
    "  ds_Xy_train = pd.concat([y_train, X_train], axis=1)\n",
    "  ds_Xy_test = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "  Dataset_Xy_train = Dataset.from_pandas(ds_Xy_train)\n",
    "  Dataset_Xy_test = Dataset.from_pandas(ds_Xy_test)\n",
    "  Dataset_Xy_train, Dataset_Xy_test\n",
    "\n",
    "  def tokenize(batch):\n",
    "    return tokenizer(batch[\"sequence\"], return_tensors=\"pt\", padding='longest', max_length=30, truncation=True)\n",
    "\n",
    "  Dataset_Xy_train_tok = Dataset_Xy_train.map(tokenize, batched=True, batch_size=None)\n",
    "  new_column = [\"train\"] * len(Dataset_Xy_train_tok)\n",
    "  Dataset_Xy_train_tok = Dataset_Xy_train_tok.add_column(\"dset\", new_column)\n",
    "\n",
    "  Dataset_Xy_test_tok = Dataset_Xy_test.map(tokenize, batched=True, batch_size=None)\n",
    "  new_column = [\"test\"] * len(Dataset_Xy_test_tok)\n",
    "  Dataset_Xy_test_tok = Dataset_Xy_test_tok.add_column(\"dset\", new_column)\n",
    "\n",
    "  print(Dataset_Xy_train_tok['input_ids'][:2], file=sys.stdout)\n",
    "  print(Dataset_Xy_train_tok['attention_mask'][:2], file=sys.stdout)\n",
    "  \n",
    "  dds = DatasetDict({\n",
    "    'train': Dataset_Xy_train_tok,\n",
    "    'test': Dataset_Xy_test_tok\n",
    "  })\n",
    "\n",
    "# switch to GPU\n",
    "\n",
    "  if torch.cuda.device_count() > 0:\n",
    "    model_cls.to('cuda')\n",
    "\n",
    "# train model\n",
    "\n",
    "  output_dir = path_prefix + 'outputs'\n",
    "\n",
    "  args = TrainingArguments(output_dir, learning_rate=lr, warmup_steps=warmup, fp16=True,\n",
    "    evaluation_strategy=\"steps\", per_device_train_batch_size=train_bs, per_device_eval_batch_size=eval_bs, \n",
    "    eval_steps=eval_steps, save_steps=save_steps, logging_steps=100000, save_total_limit=save_total_limit,\n",
    "    load_best_model_at_end=True, num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
    "\n",
    "  def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"accuracy\",\"precision\",\"recall\",\"f1\",\"matthews_correlation\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "  trainer = Trainer(model_cls, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "\n",
    "  trainer.train();\n",
    "\n",
    "# save model\n",
    "\n",
    "  fpath = path_prefix + \"DNABERT/Output_Models/\" + fname.split(\".\")[0] + \"/\"\n",
    "  print(fpath, file=sys.stdout)\n",
    "  trainer.save_model(fpath)\n",
    "\n",
    "# test model\n",
    "\n",
    "  eval_preds = trainer.predict(dds['test'])\n",
    "\n",
    "  print(eval_preds, file=sys.stdout)\n",
    "\n",
    "  tfbs_ds = fname.split(\".\")[0] \n",
    "  testing_results_df.loc[tfbs_ds] = [eval_preds.metrics['test_loss'],\n",
    "                                  eval_preds.metrics['test_accuracy'],\n",
    "                                  eval_preds.metrics['test_precision'],\n",
    "                                  eval_preds.metrics['test_recall'],\n",
    "                                  eval_preds.metrics['test_f1'],\n",
    "                                  eval_preds.metrics['test_matthews_correlation'],\n",
    "                                  eval_preds.metrics['test_runtime']]\n",
    "\n",
    "outpath = path_prefix + \"DNABERT/output/\" + run_name + \".csv\"\n",
    "testing_results_df.to_csv(outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
