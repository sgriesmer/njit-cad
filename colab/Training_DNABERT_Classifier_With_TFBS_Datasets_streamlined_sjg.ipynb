{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgriesmer/njit-cad/blob/main/Training_DNABERT_Classifier_With_TFBS_Datasets_streamlined_sjg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfoAktdfUsl1"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSKLjlh-HmIi"
      },
      "source": [
        "Installing Bio, transformers, genomic-benchmarks, and datasets packages.  The Bio package is from Biopython; transformers package for machine learning (pytorch, tensorflow); genomic-benchmarks and datasets from ML-Bioinfo-CEITEC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTrn2sqTOGNV",
        "outputId": "43865a08-55b5-46d0-8a58-652727edc99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for genomic-benchmarks (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq Bio transformers genomic-benchmarks datasets transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se-0kxjeM_Li"
      },
      "source": [
        "Access TFBS dataset in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SC8phKyD_uH",
        "outputId": "265e7c82-3edc-42ff-932f-5b97f49cab68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2jXeMkTnSE8"
      },
      "source": [
        "Run model on next 10 datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "c52111082c994800ad2e73f9a1006898",
            "c3bf40e9123646baa0252cee30f92ee9",
            "349d2d68ac8040098143dee02fd4cb25",
            "4fad3f6bbdd943fca89b2aef05ef1b50",
            "154f733234094324a6bc77aa626d838e",
            "eb2509a5f9404b3b9f1132e965b5ffab",
            "fbde223bd28b4a66b0238a3ebb751219",
            "a69fea58420a4f1fbd65d6660145c2b0"
          ]
        },
        "id": "9eGhbThTEVTP",
        "outputId": "7f4cf3b2-61a8-4f5b-c264-69ea76c84537"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c52111082c994800ad2e73f9a1006898",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3bf40e9123646baa0252cee30f92ee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/359M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at armheb/DNA_bert_6 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349d2d68ac8040098143dee02fd4cb25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fad3f6bbdd943fca89b2aef05ef1b50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "154f733234094324a6bc77aa626d838e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb2509a5f9404b3b9f1132e965b5ffab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/77988 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbde223bd28b4a66b0238a3ebb751219",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25996 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8646' max='9752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8646/9752 28:48 < 03:41, 5.00 it/s, Epoch 3.55/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.296700</td>\n",
              "      <td>0.253589</td>\n",
              "      <td>0.902293</td>\n",
              "      <td>0.898950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.270653</td>\n",
              "      <td>0.905716</td>\n",
              "      <td>0.906788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.173200</td>\n",
              "      <td>0.287622</td>\n",
              "      <td>0.901216</td>\n",
              "      <td>0.904265</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a69fea58420a4f1fbd65d6660145c2b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9752' max='9752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9752/9752 32:54, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.296700</td>\n",
              "      <td>0.253589</td>\n",
              "      <td>0.902293</td>\n",
              "      <td>0.898950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.270653</td>\n",
              "      <td>0.905716</td>\n",
              "      <td>0.906788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.173200</td>\n",
              "      <td>0.287622</td>\n",
              "      <td>0.901216</td>\n",
              "      <td>0.904265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.117900</td>\n",
              "      <td>0.407823</td>\n",
              "      <td>0.889945</td>\n",
              "      <td>0.895473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# initialize parameters\n",
        "\n",
        "for fname in [\"HaibA549Yy1cV0422111Etoh02UniPk.csv\",\n",
        "  \"HaibA549Zbtb33V0422111Etoh02UniPk.csv\",\n",
        "  \"HaibEcc1CtcfcV0416102Dm002p1hUniPk.csv\",\n",
        "  \"HaibEcc1EraaV0416102Bpa1hUniPk.csv\",\n",
        "  \"HaibEcc1EralphaaV0416102Est10nm1hUniPk.csv\",\n",
        "  \"HaibEcc1EralphaaV0416102Gen1hUniPk.csv\",\n",
        "  \"HaibEcc1Foxa1sc6553V0416102Dm002p1hUniPk.csv\",\n",
        "  \"HaibEcc1GrV0416102Dex100nmUniPk.csv\",\n",
        "  \"HaibEcc1Pol2V0416102Dm002p1hUniPk.csv\",\n",
        "  \"HaibGm12878Atf2sc81188V0422111UniPk.csv\"]:\n",
        "  # \"HaibGm12878Atf3Pcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878BatfPcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878Bcl11aPcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878Bcl3V0416101UniPk.csv\",\n",
        "  # \"HaibGm12878Bclaf101388V0416101UniPk.csv\",\n",
        "  # \"HaibGm12878Cebpbsc150V0422111UniPk.csv\",\n",
        "  # \"HaibGm12878Ebf1sc137065Pcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878Egr1Pcr2xUniPk.csv\",\n",
        "  # \"HaibGm12878Elf1sc631V0416101UniPk.csv\",\n",
        "  # \"HaibGm12878Ets1Pcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878Foxm1sc502V0422111UniPk.csv\",\n",
        "  # \"HaibGm12878GabpPcr2xUniPk.csv\",\n",
        "  # \"HaibGm12878Irf4sc6059Pcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878Mef2aPcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878Mef2csc13268V0416101UniPk.csv\",\n",
        "  # \"HaibGm12878Mta3sc81325V0422111UniPk.csv\",\n",
        "  # \"HaibGm12878Nfatc1sc17834V0422111UniPk.csv\",\n",
        "  # \"HaibGm12878Nficsc81335V0422111UniPk.csv\",\n",
        "  # \"HaibGm12878NrsfPcr1xUniPk.csv\",\n",
        "  # \"HaibGm12878P300Pcr1xUniPk.csv\"]:\n",
        "\n",
        "  dsname = \"/content/drive/MyDrive/DNABERT/Datasets/tfbs/\" + fname\n",
        "  tfbs_dataset = pd.read_csv(dsname, sep=',')\n",
        "\n",
        "# change standard error and output to saved output file\n",
        "\n",
        "  stdpath = \"/content/drive/MyDrive/DNABERT/output/\" + fname.split(\".\")[0] + \"/\"\n",
        "\n",
        "  try:\n",
        "    os.makedirs(stdpath)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "\n",
        "  stdpath_out = stdpath + \"stdout.txt\"\n",
        "  stdpath_err = stdpath + \"stderr.txt\"\n",
        "\n",
        "  sys.stdout = open(stdpath_out, 'w')\n",
        "  sys.stderr = open(stdpath_err, 'w')\n",
        "\n",
        "# print dataset shape and initial values\n",
        "\n",
        "  print(\"tfbs dataset:\", tfbs_dataset.shape, file=sys.stdout)\n",
        "  print(\"tfbs dataset initial values:\", tfbs_dataset.head(), file=sys.stdout)\n",
        "\n",
        "# reformat input\n",
        "\n",
        "  column_names = [\"labels\", \"seq\"]\n",
        "  tfbs_dataset_res = pd.DataFrame(columns=column_names)\n",
        "\n",
        "  j = 1\n",
        "  for i in range(tfbs_dataset.count()['names']):\n",
        "    name = tfbs_dataset['names'][i]\n",
        "    pos_seq = tfbs_dataset['seq'][i]\n",
        "    neg_seq = tfbs_dataset['neg_seq'][i]\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    tfbs_dataset_res.loc[j] = [pos_label, pos_seq]\n",
        "    tfbs_dataset_res.loc[j+1] = [neg_label, neg_seq]\n",
        "    j+=2\n",
        "\n",
        "  X = tfbs_dataset_res['seq']\n",
        "  y = tfbs_dataset_res['labels']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# print size of training and test sets\n",
        "\n",
        "  print(\"X_train shape:\", X_train.shape, file=sys.stdout)\n",
        "  print(\"y_train shape:\", y_train.shape, file=sys.stdout)\n",
        "  print(\"X_test shape:\", X_test.shape, file=sys.stdout)\n",
        "  print(\"y_test shape:\", y_test.shape, file=sys.stdout)\n",
        "\n",
        "# tokenization\n",
        "\n",
        "  def kmers_stride1(s, k=6):\n",
        "    return [s[i:i + k] for i in range(0, len(s)-k+1)]\n",
        "\n",
        "# load pre-trained model\n",
        "\n",
        "  model_cls = AutoModelForSequenceClassification.from_pretrained(\"armheb/DNA_bert_6\", num_labels=2)\n",
        "  params = list(model_cls.named_parameters())\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"armheb/DNA_bert_6\")\n",
        "\n",
        "# reformat data to Hugging Face Dataset format from pandas\n",
        "\n",
        "  ds_Xy_train = pd.concat([y_train, X_train], axis=1)\n",
        "  ds_Xy_test = pd.concat([y_test, X_test], axis=1)\n",
        "\n",
        "  Dataset_Xy_train = Dataset.from_pandas(ds_Xy_train)\n",
        "  Dataset_Xy_test = Dataset.from_pandas(ds_Xy_test)\n",
        "  Dataset_Xy_train, Dataset_Xy_test\n",
        "\n",
        "  def tok_func(x): return tokenizer(\" \".join(kmers_stride1(x[\"seq\"])))\n",
        "\n",
        "  Dataset_Xy_train_tok = Dataset_Xy_train.map(tok_func, batched=False)\n",
        "  new_column = [\"train\"] * len(Dataset_Xy_train_tok)\n",
        "  Dataset_Xy_train_tok = Dataset_Xy_train_tok.add_column(\"dset\", new_column)\n",
        "\n",
        "  Dataset_Xy_test_tok = Dataset_Xy_test.map(tok_func, batched=False)\n",
        "  new_column = [\"test\"] * len(Dataset_Xy_test_tok)\n",
        "  Dataset_Xy_test_tok = Dataset_Xy_test_tok.add_column(\"dset\", new_column)\n",
        "\n",
        "  dds = DatasetDict({\n",
        "    'train': Dataset_Xy_train_tok,\n",
        "    'test': Dataset_Xy_test_tok\n",
        "  })\n",
        "\n",
        "# switch to GPU\n",
        "\n",
        "  if torch.cuda.device_count() > 0:\n",
        "    model_cls.to('cuda')\n",
        "\n",
        "# train model\n",
        "\n",
        "  bs = 32\n",
        "  epochs = 4\n",
        "  lr = 8e-5\n",
        "\n",
        "  args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
        "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  trainer = Trainer(model_cls, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "  trainer.train();\n",
        "\n",
        "# save model\n",
        "\n",
        "  fpath = \"/content/drive/MyDrive/DNABERT/Output_Models/\" + fname.split(\".\")[0] + \"/\"\n",
        "  print(fpath, file=sys.stdout)\n",
        "  trainer.save_model(fpath)\n",
        "\n",
        "# test model\n",
        "\n",
        "  eval_preds = trainer.predict(dds['test'])\n",
        "\n",
        "  print(eval_preds, file=sys.stdout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNxH_dZNx7Zh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
